{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is  2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as li\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"TensorFlow version is \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\1.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\10.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\11.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\12.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\13.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\14.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\15.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\16.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\17.wav\n",
      "F:\\TF\\AudioRec\\AudioData\\5 Мира\\1 Мира\\18.wav\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_MIRA = 'AudioData/5 Мира'\n",
    "\n",
    "paths = li.util.find_files(PATH_TO_MIRA)\n",
    "print(len(paths))\n",
    "print('\\n'.join(paths[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(path, sr=16000):\n",
    "    example = li.load(path, sr=sr)[0]\n",
    "    if example.size < sr:\n",
    "        example = np.concatenate((example, np.zeros(sr-example.size)))\n",
    "    else:\n",
    "        example = example[:sr]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(paths, num_threads=4):\n",
    "    with ThreadPoolExecutor(num_threads) as pool:\n",
    "        data = list(pool.map(get_audio, paths))\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(paths, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, 16000), types: tf.float64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(70).batch(BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 16000, 10)         40        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 4000, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1000, 1)           40        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1000, 1)           4         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100)               30900     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 31,085\n",
      "Trainable params: 31,083\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer((16000)),\n",
    "    tf.keras.layers.Reshape((16000, 1)),\n",
    "    tf.keras.layers.Conv1D(filters=10, kernel_size=4, strides=1, padding='same', use_bias=False),\n",
    "    tf.keras.layers.MaxPool1D(4),\n",
    "    tf.keras.layers.Conv1D(filters=1, kernel_size=4, strides=4, use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('tanh'),\n",
    "    tf.keras.layers.GRU(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "], name='discriminator')\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_18 (Reshape)         (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 100)               30900     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              100000    \n",
      "_________________________________________________________________\n",
      "reshape_19 (Reshape)         (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_21 (Conv1DT (None, 1000, 1)           2         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_22 (Conv1DT (None, 2000, 1)           2         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_23 (Conv1DT (None, 4000, 1)           2         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_24 (Conv1DT (None, 4000, 1)           1         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_25 (Conv1DT (None, 8000, 1)           2         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_26 (Conv1DT (None, 16000, 1)          2         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_27 (Conv1DT (None, 16000, 1)          1         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 16000)             0         \n",
      "=================================================================\n",
      "Total params: 130,912\n",
      "Trainable params: 130,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer((100,)),\n",
    "    tf.keras.layers.Reshape((100, 1)),\n",
    "    tf.keras.layers.GRU(100),\n",
    "    tf.keras.layers.Dense(1000, activation='relu', use_bias=False),\n",
    "    tf.keras.layers.Reshape((1000, 1)),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=2, strides=1, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=2, strides=2, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=2, strides=2, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=1, strides=1, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=2, strides=2, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=2, strides=2, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Conv1DTranspose(filters=1, kernel_size=1, strides=1, padding='same', use_bias=False),\n",
    "    tf.keras.layers.Activation('tanh'),\n",
    "    tf.keras.layers.Reshape((16000,))\n",
    "], name='generator')\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "noise = tf.random.normal((10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 16000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = generator(noise)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = loss(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = loss(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return loss(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optim = tf.keras.optimizers.Adam()\n",
    "disc_optim = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(train_batch):\n",
    "    noise = tf.random.normal([BATCH_SIZE, latent_dim])\n",
    "    \n",
    "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "        generated_audio = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(train_batch, training=True)\n",
    "        fake_output = discriminator(generated_audio, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "    gen_grad = g_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    disc_grad = d_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    gen_optim.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
    "    disc_optim.apply_gradients(zip(disc_grad, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    start = time.time()\n",
    "    for i in range(epochs):\n",
    "        for train_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(train_batch)\n",
    "            \n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch {:4d}: gen_loss - {:.4f} | disc_loss - {:.4f} | {:.3f} seconds'.format(\n",
    "                i+1, gen_loss, disc_loss, time.time()-start))\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   10: gen_loss - 0.1849 | disc_loss - 2.5704 | 109.055 seconds\n",
      "Epoch   20: gen_loss - 0.9182 | disc_loss - 1.3200 | 105.074 seconds\n",
      "Epoch   30: gen_loss - 1.5014 | disc_loss - 1.0131 | 104.334 seconds\n",
      "Epoch   40: gen_loss - 4.9703 | disc_loss - 0.2124 | 103.713 seconds\n",
      "Epoch   50: gen_loss - 5.9062 | disc_loss - 0.1348 | 105.061 seconds\n",
      "Epoch   60: gen_loss - 7.0383 | disc_loss - 0.0233 | 106.237 seconds\n",
      "Epoch   70: gen_loss - 0.1004 | disc_loss - 2.9351 | 104.147 seconds\n",
      "Epoch   80: gen_loss - 1.3921 | disc_loss - 0.8540 | 105.886 seconds\n",
      "Epoch   90: gen_loss - 2.0917 | disc_loss - 0.6234 | 104.559 seconds\n",
      "Epoch  100: gen_loss - 2.2293 | disc_loss - 1.0224 | 106.526 seconds\n",
      "Epoch  110: gen_loss - 0.5751 | disc_loss - 1.4958 | 106.066 seconds\n",
      "Epoch  120: gen_loss - 0.6786 | disc_loss - 1.5173 | 106.182 seconds\n",
      "Epoch  130: gen_loss - 0.8337 | disc_loss - 1.1043 | 109.148 seconds\n",
      "Epoch  140: gen_loss - 0.5793 | disc_loss - 1.5160 | 108.361 seconds\n",
      "Epoch  150: gen_loss - 0.2436 | disc_loss - 1.6478 | 106.600 seconds\n",
      "Epoch  160: gen_loss - 1.0471 | disc_loss - 0.9778 | 106.962 seconds\n",
      "Epoch  170: gen_loss - 1.4113 | disc_loss - 0.7046 | 110.062 seconds\n",
      "Epoch  180: gen_loss - 0.6393 | disc_loss - 1.1016 | 120.555 seconds\n",
      "Epoch  190: gen_loss - 0.8008 | disc_loss - 1.0400 | 145.967 seconds\n",
      "Epoch  200: gen_loss - 0.8986 | disc_loss - 0.9690 | 145.873 seconds\n",
      "Epoch  210: gen_loss - 0.9542 | disc_loss - 0.9107 | 145.783 seconds\n",
      "Epoch  220: gen_loss - 0.9444 | disc_loss - 0.9051 | 144.515 seconds\n",
      "Epoch  230: gen_loss - 1.0411 | disc_loss - 0.8823 | 144.254 seconds\n",
      "Epoch  240: gen_loss - 1.0485 | disc_loss - 0.8480 | 144.252 seconds\n",
      "Epoch  250: gen_loss - 1.0179 | disc_loss - 0.8517 | 144.054 seconds\n",
      "Epoch  260: gen_loss - 0.0417 | disc_loss - 3.6592 | 143.918 seconds\n",
      "Epoch  270: gen_loss - 0.2964 | disc_loss - 2.5375 | 144.706 seconds\n",
      "Epoch  280: gen_loss - 0.4138 | disc_loss - 2.2368 | 143.371 seconds\n",
      "Epoch  290: gen_loss - 0.4903 | disc_loss - 1.9787 | 144.352 seconds\n",
      "Epoch  300: gen_loss - 0.5838 | disc_loss - 1.7216 | 144.793 seconds\n",
      "Epoch  310: gen_loss - 0.6804 | disc_loss - 1.4995 | 144.795 seconds\n",
      "Epoch  320: gen_loss - 0.7638 | disc_loss - 1.3330 | 144.615 seconds\n",
      "Epoch  330: gen_loss - 0.8289 | disc_loss - 1.2100 | 145.583 seconds\n",
      "Epoch  340: gen_loss - 0.8807 | disc_loss - 1.1259 | 144.861 seconds\n",
      "Epoch  350: gen_loss - 0.9482 | disc_loss - 1.0503 | 144.789 seconds\n",
      "Epoch  360: gen_loss - 1.0143 | disc_loss - 0.9568 | 145.077 seconds\n",
      "Epoch  370: gen_loss - 1.0843 | disc_loss - 0.8866 | 146.004 seconds\n",
      "Epoch  380: gen_loss - 1.1539 | disc_loss - 0.8074 | 144.737 seconds\n",
      "Epoch  390: gen_loss - 1.2206 | disc_loss - 0.7512 | 145.243 seconds\n",
      "Epoch  400: gen_loss - 1.2919 | disc_loss - 0.6556 | 145.002 seconds\n",
      "Epoch  410: gen_loss - 1.3724 | disc_loss - 0.6163 | 145.088 seconds\n",
      "Epoch  420: gen_loss - 1.4568 | disc_loss - 0.5609 | 145.080 seconds\n",
      "Epoch  430: gen_loss - 1.5418 | disc_loss - 0.4987 | 145.005 seconds\n",
      "Epoch  440: gen_loss - 1.6306 | disc_loss - 0.4616 | 145.741 seconds\n",
      "Epoch  450: gen_loss - 1.7230 | disc_loss - 0.4083 | 145.042 seconds\n",
      "Epoch  460: gen_loss - 1.8161 | disc_loss - 0.3637 | 144.784 seconds\n",
      "Epoch  470: gen_loss - 1.8196 | disc_loss - 0.3502 | 144.140 seconds\n",
      "Epoch  480: gen_loss - 0.1033 | disc_loss - 3.0408 | 144.402 seconds\n",
      "Epoch  490: gen_loss - 0.4846 | disc_loss - 1.9591 | 143.886 seconds\n",
      "Epoch  500: gen_loss - 0.7478 | disc_loss - 1.4421 | 144.122 seconds\n",
      "Epoch  510: gen_loss - 0.9149 | disc_loss - 1.1162 | 143.590 seconds\n",
      "Epoch  520: gen_loss - 0.8144 | disc_loss - 1.1288 | 145.222 seconds\n",
      "Epoch  530: gen_loss - 0.7743 | disc_loss - 1.1739 | 114.285 seconds\n",
      "Epoch  540: gen_loss - 0.7808 | disc_loss - 1.1686 | 104.546 seconds\n",
      "Epoch  550: gen_loss - 0.7837 | disc_loss - 1.1717 | 103.185 seconds\n",
      "Epoch  560: gen_loss - 0.7893 | disc_loss - 1.1767 | 101.850 seconds\n",
      "Epoch  570: gen_loss - 0.7918 | disc_loss - 1.1609 | 102.514 seconds\n",
      "Epoch  580: gen_loss - 0.7972 | disc_loss - 1.1586 | 101.196 seconds\n",
      "Epoch  590: gen_loss - 0.7981 | disc_loss - 1.1640 | 104.082 seconds\n",
      "Epoch  600: gen_loss - 0.7461 | disc_loss - 1.1912 | 103.410 seconds\n",
      "Epoch  610: gen_loss - 0.7706 | disc_loss - 1.1898 | 102.888 seconds\n",
      "Epoch  620: gen_loss - 0.7723 | disc_loss - 1.1870 | 107.164 seconds\n",
      "Epoch  630: gen_loss - 0.7799 | disc_loss - 1.1815 | 143.928 seconds\n",
      "Epoch  640: gen_loss - 0.7830 | disc_loss - 1.1859 | 144.354 seconds\n",
      "Epoch  650: gen_loss - 0.7855 | disc_loss - 1.1633 | 145.103 seconds\n",
      "Epoch  660: gen_loss - 0.7815 | disc_loss - 1.1758 | 143.906 seconds\n",
      "Epoch  670: gen_loss - 0.7697 | disc_loss - 1.1977 | 144.335 seconds\n",
      "Epoch  680: gen_loss - 0.7537 | disc_loss - 1.2253 | 143.252 seconds\n",
      "Epoch  690: gen_loss - 0.7602 | disc_loss - 1.2291 | 143.911 seconds\n",
      "Epoch  700: gen_loss - 0.7581 | disc_loss - 1.2171 | 145.115 seconds\n",
      "Epoch  710: gen_loss - 0.7221 | disc_loss - 1.2826 | 145.746 seconds\n",
      "Epoch  720: gen_loss - 0.7155 | disc_loss - 1.3263 | 145.602 seconds\n",
      "Epoch  730: gen_loss - 0.7043 | disc_loss - 1.3419 | 145.892 seconds\n",
      "Epoch  740: gen_loss - 0.6983 | disc_loss - 1.3590 | 145.081 seconds\n",
      "Epoch  750: gen_loss - 0.6974 | disc_loss - 1.3679 | 145.622 seconds\n",
      "Epoch  760: gen_loss - 0.6987 | disc_loss - 1.3634 | 145.564 seconds\n",
      "Epoch  770: gen_loss - 0.6999 | disc_loss - 1.3562 | 145.550 seconds\n",
      "Epoch  780: gen_loss - 0.7018 | disc_loss - 1.3816 | 145.476 seconds\n",
      "Epoch  790: gen_loss - 0.7034 | disc_loss - 1.3737 | 144.786 seconds\n",
      "Epoch  800: gen_loss - 0.7053 | disc_loss - 1.3585 | 110.863 seconds\n",
      "Epoch  810: gen_loss - 0.7076 | disc_loss - 1.3535 | 103.655 seconds\n",
      "Epoch  820: gen_loss - 0.7109 | disc_loss - 1.3691 | 105.641 seconds\n",
      "Epoch  830: gen_loss - 0.7138 | disc_loss - 1.3178 | 105.222 seconds\n",
      "Epoch  840: gen_loss - 0.7180 | disc_loss - 1.3396 | 102.248 seconds\n",
      "Epoch  850: gen_loss - 0.7229 | disc_loss - 1.3136 | 102.527 seconds\n",
      "Epoch  860: gen_loss - 0.7303 | disc_loss - 1.3083 | 105.557 seconds\n",
      "Epoch  870: gen_loss - 0.7380 | disc_loss - 1.2879 | 105.105 seconds\n",
      "Epoch  880: gen_loss - 0.7478 | disc_loss - 1.2678 | 103.091 seconds\n",
      "Epoch  890: gen_loss - 0.7591 | disc_loss - 1.2336 | 104.959 seconds\n",
      "Epoch  900: gen_loss - 0.7773 | disc_loss - 1.2391 | 103.857 seconds\n",
      "Epoch  910: gen_loss - 0.7950 | disc_loss - 1.1431 | 105.442 seconds\n",
      "Epoch  920: gen_loss - 0.8251 | disc_loss - 1.1846 | 105.643 seconds\n",
      "Epoch  930: gen_loss - 0.8554 | disc_loss - 1.0821 | 105.925 seconds\n",
      "Epoch  940: gen_loss - 0.9028 | disc_loss - 1.0076 | 106.842 seconds\n",
      "Epoch  950: gen_loss - 0.9683 | disc_loss - 1.0793 | 102.366 seconds\n",
      "Epoch  960: gen_loss - 1.0485 | disc_loss - 0.8784 | 141.909 seconds\n",
      "Epoch  970: gen_loss - 1.1375 | disc_loss - 0.6574 | 143.010 seconds\n",
      "Epoch  980: gen_loss - 1.2200 | disc_loss - 0.5576 | 143.246 seconds\n",
      "Epoch  990: gen_loss - 1.3180 | disc_loss - 0.5779 | 143.094 seconds\n",
      "Epoch 1000: gen_loss - 1.3783 | disc_loss - 0.5660 | 143.059 seconds\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"models/generative/generators/generator-v0.2.h5\")\n",
    "discriminator.save(\"models/generative/discriminators/generator-v0.2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiorec",
   "language": "python",
   "name": "audiorec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
